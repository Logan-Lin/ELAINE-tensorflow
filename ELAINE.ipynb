{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of ELAINE\n",
    "\n",
    "_Goyal P, Hosseinmardi H, Ferrara E, et al. Capturing Edge Attributes via Network Embedding[J]._ arXiv preprint arXiv:1805.03280, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce of Variational Autoencoder \n",
    "\n",
    "> Codes in this section are from [Variational Autoencoder in TensorFlow](https://jmetzen.github.io/2015-11-27/vae.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# np.random.seed(0)\n",
    "# tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-147-1eedde87f71e>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /data/anaconda3/envs/linyan_3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /data/anaconda3/envs/linyan_3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /data/anaconda3/envs/linyan_3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /data/anaconda3/envs/linyan_3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /data/anaconda3/envs/linyan_3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "n_samples = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of VAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "\n",
    "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
    "    end-to-end.\n",
    "\n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus,\n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(\n",
    "            tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "\n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and\n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "\n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        network_weights = self._initialize_weights(**self.network_architecture)\n",
    "\n",
    "        # Use recognition network to determine mean and\n",
    "        # (log) variance of Gaussian distribution in latent\n",
    "        # space\n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self._recognition_network(network_weights[\"weights_recog\"],\n",
    "                                      network_weights[\"biases_recog\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1,\n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma*epsilon\n",
    "        self.z = tf.add(self.z_mean,\n",
    "                        tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        # Use generator to determine mean of\n",
    "        # Bernoulli distribution of reconstructed input\n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(network_weights[\"weights_gener\"],\n",
    "                                    network_weights[\"biases_gener\"])\n",
    "\n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2,\n",
    "                            n_hidden_gener_1,  n_hidden_gener_2,\n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights['weights_recog'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
    "        all_weights['biases_recog'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "        all_weights['weights_gener'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
    "        all_weights['biases_gener'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "        return all_weights\n",
    "\n",
    "    def _recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network), which\n",
    "        # maps inputs onto a normal distribution in latent space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']),\n",
    "                                           biases['b1']))\n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']),\n",
    "                                           biases['b2']))\n",
    "        z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "                        biases['out_mean'])\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.add(tf.matmul(layer_2, weights['out_log_sigma']),\n",
    "                   biases['out_log_sigma'])\n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network), which\n",
    "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']),\n",
    "                                           biases['b1']))\n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']),\n",
    "                                           biases['b2']))\n",
    "        x_reconstr_mean = \\\n",
    "            tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "                                 biases['out_mean']))\n",
    "        return x_reconstr_mean\n",
    "\n",
    "    def _create_loss_optimizer(self):\n",
    "        # The loss is composed of two terms:\n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli distribution\n",
    "        #     induced by the decoder in the data space).\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for reconstructing the input when the activation in latent\n",
    "        #     is given.\n",
    "        # Adding 1e-10 to avoid evaluation of log(0.0)\n",
    "        reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
    "                           + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean), 1)\n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence\n",
    "        # between the distribution in latent space induced by the encoder on\n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq\n",
    "                                           - tf.square(self.z_mean)\n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "        self.cost = tf.reduce_mean(\n",
    "            reconstr_loss + latent_loss)   # average over batch\n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(\n",
    "                learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "\n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost),\n",
    "                                  feed_dict={self.x: X})\n",
    "        return cost\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run((self.z, self.z_mean), feed_dict={self.x: X})\n",
    "\n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "\n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.x_reconstr_mean,\n",
    "                             feed_dict={self.z: z_mu})\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean,\n",
    "                             feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def train(network_architecture, learning_rate=0.001,\n",
    "          batch_size=100, training_epochs=10, display_step=5):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "            cost = vae.partial_fit(batch_xs)\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/linyan_3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 174.889558022\n",
      "Epoch: 0006 cost= 113.076514144\n",
      "Epoch: 0011 cost= 108.449749132\n",
      "Epoch: 0016 cost= 106.194548173\n",
      "Epoch: 0021 cost= 104.721634438\n",
      "Epoch: 0026 cost= 103.706215737\n",
      "Epoch: 0031 cost= 102.791683003\n",
      "Epoch: 0036 cost= 102.071457228\n",
      "Epoch: 0041 cost= 101.440400030\n",
      "Epoch: 0046 cost= 100.978376118\n",
      "Epoch: 0051 cost= 100.555557112\n",
      "Epoch: 0056 cost= 100.126572016\n",
      "Epoch: 0061 cost= 99.805663480\n",
      "Epoch: 0066 cost= 99.487266499\n",
      "Epoch: 0071 cost= 99.272521210\n",
      "Epoch: 0076 cost= 99.011139346\n",
      "Epoch: 0081 cost= 98.790896287\n",
      "Epoch: 0086 cost= 98.577656583\n",
      "Epoch: 0091 cost= 98.404802690\n",
      "Epoch: 0096 cost= 98.210945199\n"
     ]
    }
   ],
   "source": [
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=500, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=250, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=250, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
    "         n_input=784, # MNIST data input (img shape: 28*28)\n",
    "         n_z=8)  # dimensionality of latent space\n",
    "\n",
    "vae = train(network_architecture, training_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = mnist.test.next_batch(100)[0]\n",
    "x_reconstruct = vae.reconstruct(x_sample)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(5):\n",
    "\n",
    "    plt.subplot(5, 2, 2*i + 1)\n",
    "    plt.imshow(x_sample[i].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Test input\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(5, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[i].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.title(\"Reconstruction\")\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrating latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data using latent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = ny = 20\n",
    "\n",
    "canvas = np.empty((28*ny, 28*nx))\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        input_vector = np.random.random_sample((2)) * 6 - 3\n",
    "        z_mu = np.array([input_vector]*vae.batch_size)\n",
    "        x_mean = vae.generate(z_mu)\n",
    "        canvas[(nx-i-1)*28:(nx-i)*28, j*28:(j+1)*28] = x_mean[0].reshape(28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = ny = 30\n",
    "x_values = np.linspace(-3, 3, nx)\n",
    "y_values = np.linspace(-3, 3, ny)\n",
    "\n",
    "canvas = np.empty((28*ny, 28*nx))\n",
    "for i, yi in enumerate(x_values):\n",
    "    for j, xi in enumerate(y_values):\n",
    "        z_mu = np.array([[xi, yi]]*vae.batch_size)\n",
    "        x_mean = vae.generate(z_mu)\n",
    "        canvas[(nx-i-1)*28:(nx-i)*28, j*28:(j+1)*28] = x_mean[0].reshape(28, 28)\n",
    "\n",
    "plt.figure(figsize=(10, 12))        \n",
    "Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrate 2-dimensional latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample, y_sample = mnist.test.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, z_mu = vae.transform(x_sample)\n",
    "plt.figure(figsize=(12, 8)) \n",
    "plt.scatter(z_mu[:, 0], z_mu[:, 1], c=np.argmax(y_sample, 1))\n",
    "plt.colorbar()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load O-D set and K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'chengdu', 'od_20161001.pkl'), 'rb') as od_file:\n",
    "    od_set = pickle.load(od_file)\n",
    "with open('model/kmeans.pkl', 'rb') as kmeans_file:\n",
    "    kmeans = pickle.load(kmeans_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_set_label = pd.DataFrame(od_set, copy=True)\n",
    "od_set_label['cluster'] = kmeans.predict(od_set[['longitude', 'latitude']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding dataset to graph: 100%|██████████| 207107/207107.0 [02:10<00:00, 1585.75it/s]\n"
     ]
    }
   ],
   "source": [
    "graph = nx.DiGraph()\n",
    "utils.add_dataset_to_graph(od_set_label, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding dataset to graph: 100%|██████████| 207107/207107 [02:55<00:00, 1177.57it/s]\n"
     ]
    }
   ],
   "source": [
    "slot_graph = nx.DiGraph()\n",
    "utils.add_timeslot_weight_to_graph(od_set_label, slot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/chengdu/graph/g_20161001.pkl', 'rb') as g_file:\n",
    "    graph = pickle.load(g_file)\n",
    "with open('data/chengdu/graph/sg_20161001.pkl', 'rb') as sg_file:\n",
    "    slot_graph = pickle.load(sg_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 400/400 [00:12<00:00, 54.91it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 1600/1600 [04:50<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "node2vec_class = Node2Vec(graph, dimensions=10, walk_length=8, num_walks=1600, weight_key='weight')\n",
    "n2v = node2vec_class.fit(window=8, min_count=1, batch_words=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of graph data\n",
    "\n",
    "Assume that there are $n$ nodes, $m$ edges, every node have $p$ attributes, and every edge have $q$ attributes.\n",
    "For every node, we fetch its $k$ nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_NUM = len(graph.nodes())\n",
    "NEIGHBOR_NUM = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge attributes matrix $E\\in \\mathbb R^{m\\times q}$\n",
    "\n",
    "In this implementation, edge attributes are weight list from slot graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr = dict()\n",
    "for start, end in slot_graph.edges():\n",
    "    edge_attr[(start, end)] = slot_graph[start][end]['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_matrix = pd.DataFrame(edge_attr).T\n",
    "edge_attr_matrix = edge_attr_matrix.sample(frac=1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_norm = pd.DataFrame(normalize(edge_attr_matrix, norm='max'))\n",
    "edge_attr_norm.index = edge_attr_matrix.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node attributes matrix $N\\in \\mathbb R^{n\\times p}$\n",
    "\n",
    "Currently, we define node attributes as their rows in the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attr = dict()\n",
    "for i in range(NODE_NUM):\n",
    "    row = []\n",
    "    for j in range(NODE_NUM):\n",
    "        try:\n",
    "            weight = graph[i][j]['weight']\n",
    "        except KeyError:\n",
    "            weight = 0\n",
    "        row.append(weight)\n",
    "    node_attr[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attr_matrix = pd.DataFrame(node_attr).T\n",
    "node_attr_matrix = node_attr_matrix.sort_index().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attr_norm = pd.DataFrame(normalize(node_attr_matrix, norm='max'))\n",
    "node_attr_norm.index = node_attr_matrix.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neighborhood matrix $F\\in \\mathbb R^{n\\times (p*(k+1))}$\n",
    "\n",
    "Every row of neighborhood matrix consists of the correspond node's attribute and its $k$ nearest neighbors' attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/linyan_3.6/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "neig = []\n",
    "for i in range(NODE_NUM):\n",
    "    row = [node_attr_matrix[i]]\n",
    "    for j in range(NEIGHBOR_NUM):\n",
    "        near_row = n2v.wv.most_similar(str(i))[j]\n",
    "        near_node = int(near_row[0])\n",
    "        row.append(node_attr_matrix.loc[near_node])\n",
    "    row_series = pd.concat(row)\n",
    "    row_series.index = range(row_series.shape[0])\n",
    "    neig.append(row_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neig_matrix = pd.DataFrame(neig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "neig_norm = pd.DataFrame(normalize(neig_matrix, norm='max'))\n",
    "neig_norm.index = neig_matrix.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELAINE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xavier Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational encoder and decoder\n",
    "\n",
    "`z_mean` and `z_log_sigma_sq` correspond to $\\mu$ and $\\sigma$ in Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_x, weights, biases, fc_activation, out_mean, out_log_sigma):\n",
    "    \"\"\"\n",
    "    VAE encoder. The length of List `weights` and `biases` indicates\n",
    "    the depth of FC layer in this encoder.\n",
    "    Maps inputs onto a normal distribution in latent space.\n",
    "    The transformation is parameterized and can be learned.\n",
    "    \n",
    "    @params input_x: Input tensor.\n",
    "    @params weights: List of FC layer weights.\n",
    "    @params biases: List of FC layer biases.\n",
    "    @params fc_activation: Activation function for FC layers.\n",
    "    @params out_mean: Dict in format {'weight': out_mean_weight, 'bias': out_mean_bias}\n",
    "    @params out_log_sigma: Dict in format {'weight': out_log_sigma_weight, 'bias': out_log_sigma_bias}\n",
    "    \"\"\"\n",
    "    current_layer = input_x\n",
    "    for weight, bias in zip(weights, biases):\n",
    "        current_layer = fc_activation(tf.add(tf.matmul(current_layer, weight), bias))\n",
    "    z_mean = tf.add(tf.matmul(current_layer, out_mean['weight']), out_mean['bias'])\n",
    "    z_log_sigma_sq = tf.add(tf.matmul(current_layer, out_log_sigma['weight']), out_log_sigma['bias'])\n",
    "    return (z_mean, z_log_sigma_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_z, weights, biases, fc_activation, out_mean):\n",
    "    \"\"\"\n",
    "    VAE decoder. The length of List `weights` and `biases` indicated\n",
    "    the depth of the FC layer in this decoder.\n",
    "    Maps points in latent space onto a Bernoulli distribution in data space.\n",
    "    The transformation is parameterized and can be learned.\n",
    "    \n",
    "    @params input_z: Input tensor sampled from Gaussian distribution.\n",
    "    @params weights: List of FC layer weights.\n",
    "    @params biases: List of FC layer biases.\n",
    "    @params fc_activation: Activation function for FC layers.\n",
    "    @params out_mean: Dict in format {'weight': out_mean_weight, 'bias': out_mean_bias}\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    current_layer = input_z\n",
    "    for weight, bias in zip(weights, biases):\n",
    "        current_layer = fc_activation(tf.add(tf.matmul(current_layer, weight), bias))\n",
    "        layers.append(current_layer)\n",
    "    x_reconstr_mean = tf.nn.sigmoid(tf.add(tf.matmul(current_layer, out_mean['weight']), out_mean['bias']))\n",
    "    return x_reconstr_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder constants\n",
    "ENCODER_INPUT_LENGTH = neig_matrix.shape[1]  # Single encoder input tensor length. Current: 2000\n",
    "ENCODER_SIZES = [ENCODER_INPUT_LENGTH, 1024, 512, 256, 128]  # Sizes of each layer in encoder FC part.\n",
    "LATENT_SIZE = 32  # Size of latent space, also the length of encoder output.\n",
    "ENCODER_DEPTH = len(ENCODER_SIZES)\n",
    "\n",
    "# Single decoder constants\n",
    "SINGLE_DECODER_INPUT_LENGTH = LATENT_SIZE\n",
    "SINGLE_DECODER_SIZES = [SINGLE_DECODER_INPUT_LENGTH, 128, 256, 512, 1024]  # Size of each layer in single decoder FC part.\n",
    "SINGLE_DECODER_OUTPUT_LENGTH = ENCODER_INPUT_LENGTH\n",
    "SINGLE_DECODER_DEPTH = len(SINGLE_DECODER_SIZES)\n",
    "\n",
    "# Double decoder constants\n",
    "DOUBLE_DECODER_INPUT_LENGTH = LATENT_SIZE * 2\n",
    "DOUBLE_DECODER_SIZES = [DOUBLE_DECODER_INPUT_LENGTH, 64, 32]  # Sizes of each layer in double decoder FC part.\n",
    "DOUBLE_DECODER_OUTPUT_LENGTH = edge_attr_matrix.shape[1]\n",
    "DOUBLE_DECODER_DEPTH = len(DOUBLE_DECODER_SIZES)\n",
    "\n",
    "# Basic model parameters\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.001\n",
    "TRAINING_EPOCHS = 100\n",
    "\n",
    "NODE_ATTR_LOSS_WEIGHT = 1\n",
    "LATENT_LOSS_WEIGHT = 0.01\n",
    "EDGE_ATTR_LOSS_WEIGHT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch graph data in a batch style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(edge_attr, batch_size, steps):\n",
    "    \"\"\"\n",
    "    Fetch next batch graph data.\n",
    "    \"\"\"\n",
    "    edge_count = edge_attr.shape[0]\n",
    "    \n",
    "    start = steps*batch_size % edge_count\n",
    "    end = (steps+1)*batch_size % edge_count\n",
    "    \n",
    "    if start > end:\n",
    "        result = pd.concat([edge_attr.iloc[start:], edge_attr.iloc[:end]])\n",
    "    else:\n",
    "        result = edge_attr.iloc[start:end]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge everything into final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core structure of ELAINE:\n",
    "![structure.png](image/structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELAINE:\n",
    "    \"\"\"\n",
    "    Implementation of ELAINE model.\n",
    "    Paper: Goyal P, Hosseinmardi H, Ferrara E, et al. Capturing Edge Attributes via Network Embedding[J]. \n",
    "    arXiv preprint arXiv:1805.03280, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, learning_rate):\n",
    "        tf.reset_default_graph()\n",
    "        # Get encoder and decoders' initial parameters.\n",
    "        self._init_params()\n",
    "\n",
    "        # Input placeholder for \"left encoder\" and \"right encoder\"\n",
    "        with tf.variable_scope('input'):\n",
    "            self.l_node_input = tf.placeholder(dtype=tf.float32,\n",
    "                                               shape=[None, ENCODER_INPUT_LENGTH], name='l_node')\n",
    "            self.r_node_input = tf.placeholder(dtype=tf.float32,\n",
    "                                               shape=[None, ENCODER_INPUT_LENGTH], name='r_node')\n",
    "\n",
    "            self.edge_attr = tf.placeholder(dtype=tf.float32,\n",
    "                                            shape=[None, DOUBLE_DECODER_OUTPUT_LENGTH], name='edge')\n",
    "\n",
    "        # Assign learning rate and batch size.\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Build network structure.\n",
    "        self._create_network()\n",
    "        # Create loss tensor and optimize operation.\n",
    "        self._create_loss_optimizer()\n",
    "\n",
    "        # Run gloabal variables initializer.\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def _create_network(self):\n",
    "        \"\"\"\n",
    "        Build up network structure.\n",
    "        \"\"\"\n",
    "        # Feed \"left\" and \"right\" node attribute matrixes into variational encoder.\n",
    "        # Use encoder to determine mean and variance of Gaussian distribution in latent space.\n",
    "        self.l_z_mean, self.l_z_log_sigma_sq = encoder(self.l_node_input,\n",
    "                                                       self.e_p['weights'], self.e_p['biases'],\n",
    "                                                       self.e_p['fc_activation'], self.e_p['out_mean'],\n",
    "                                                       self.e_p['out_log_sigma'])\n",
    "        self.r_z_mean, self.r_z_log_sigma_sq = encoder(self.r_node_input,\n",
    "                                                       self.e_p['weights'], self.e_p['biases'],\n",
    "                                                       self.e_p['fc_activation'], self.e_p['out_mean'],\n",
    "                                                       self.e_p['out_log_sigma'])\n",
    "\n",
    "        # Draw one sample form Gaussian distribution.\n",
    "        l_eps = tf.random_normal(\n",
    "            (self.batch_size, LATENT_SIZE), 0, 1, dtype=tf.float32)\n",
    "        r_eps = tf.random_normal(\n",
    "            (self.batch_size, LATENT_SIZE), 0, 1, dtype=tf.float32)\n",
    "        # z = mu = sigma * epsilon\n",
    "        l_z = tf.add(self.l_z_mean, tf.multiply(\n",
    "            tf.sqrt(tf.exp(self.l_z_log_sigma_sq)), l_eps))\n",
    "        r_z = tf.add(self.r_z_mean, tf.multiply(\n",
    "            tf.sqrt(tf.exp(self.r_z_log_sigma_sq)), r_eps))\n",
    "\n",
    "        # Use decoder to determine mean of Bernoulli distribution of reconstructed input.\n",
    "        self.l_reconstr_mean = decoder(l_z,\n",
    "                                       self.s_d_p['weights'], self.s_d_p['biases'],\n",
    "                                       self.s_d_p['fc_activation'], self.s_d_p['out_mean'])\n",
    "        self.r_reconstr_mean = decoder(r_z,\n",
    "                                       self.s_d_p['weights'], self.s_d_p['biases'],\n",
    "                                       self.s_d_p['fc_activation'], self.s_d_p['out_mean'])\n",
    "\n",
    "        # Concatenate \"left\" and \"right\" encoder's result into matrix doubled the width.\n",
    "        # The matrix is feeded into decoder and used to reconstruct edge attributes.\n",
    "        edge_input = tf.concat([l_z, r_z], -1)\n",
    "        self.edge_reconstr_mean = decoder(edge_input,\n",
    "                                          self.d_d_p['weights'], self.d_d_p['biases'],\n",
    "                                          self.d_d_p['fc_activation'], self.d_d_p['out_mean'])\n",
    "\n",
    "    def _create_loss_optimizer(self):\n",
    "        \"\"\"\n",
    "        Calculate nodes and labels' reconstruction losses and latent losses,\n",
    "        then combine them into final loss.\n",
    "        \"\"\"\n",
    "        l_reconstr_loss = -tf.reduce_sum(self.l_node_input * tf.log(1e-10 + self.l_reconstr_mean)\n",
    "                                         + (1-self.l_node_input) *\n",
    "                                         tf.log(1e-10 + 1 - self.l_reconstr_mean), 1)\n",
    "        r_reconstr_loss = -tf.reduce_sum(self.r_node_input * tf.log(1e-10 + self.r_reconstr_mean)\n",
    "                                         + (1-self.r_node_input) *\n",
    "                                         tf.log(1e-10 + 1 - self.r_reconstr_mean), 1)\n",
    "\n",
    "        l_latent_loss = -tf.reduce_sum(1 + self.l_z_log_sigma_sq\n",
    "                                       - tf.square(self.l_z_mean)\n",
    "                                       - tf.exp(self.l_z_log_sigma_sq), 1)\n",
    "        r_latent_loss = -tf.reduce_sum(1 + self.r_z_log_sigma_sq\n",
    "                                       - tf.square(self.r_z_mean)\n",
    "                                       - tf.exp(self.r_z_log_sigma_sq), 1)\n",
    "\n",
    "        edge_reconstr_loss = -tf.reduce_sum(self.edge_attr * tf.log(1e-10 + self.edge_reconstr_mean)\n",
    "                                            + (1-self.edge_attr) *\n",
    "                                            tf.log(1e-10 + 1 - self.edge_reconstr_mean), 1)\n",
    "\n",
    "        self.cost = \\\n",
    "            NODE_ATTR_LOSS_WEIGHT * tf.reduce_sum(0.5 * l_reconstr_loss + 0.5 * r_reconstr_loss) + \\\n",
    "            LATENT_LOSS_WEIGHT * tf.reduce_sum(0.5 * l_latent_loss + 0.5 * r_latent_loss) + \\\n",
    "            EDGE_ATTR_LOSS_WEIGHT * tf.reduce_sum(edge_reconstr_loss)\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "    def partial_fit(self, edge_batch):\n",
    "        \"\"\"\n",
    "        Fit a batch of edge attribute matrix.\n",
    "        \"\"\"\n",
    "        l_node_attrs, r_node_attrs, edge_attrs = [], [], []\n",
    "        for node_pair, edge_attr in edge_batch.iterrows():\n",
    "            l_node = node_pair[0]\n",
    "            r_node = node_pair[1]\n",
    "\n",
    "            l_node_attrs.append(neig_norm.loc[l_node])\n",
    "            r_node_attrs.append(neig_norm.loc[r_node])\n",
    "\n",
    "            edge_attrs.append(edge_attr)\n",
    "        l_node_attrs = np.array(l_node_attrs)\n",
    "        r_node_attrs = np.array(r_node_attrs)\n",
    "        edge_attrs = np.array(edge_attrs)\n",
    "\n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost),\n",
    "                                  feed_dict={self.l_node_input: l_node_attrs,\n",
    "                                             self.r_node_input: r_node_attrs,\n",
    "                                             self.edge_attr: edge_attrs})\n",
    "        return cost\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data by mapping it into the latent space.\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.l_z_mean, feed_dict={self.l_node_input: X})\n",
    "\n",
    "    def _init_params(self):\n",
    "        with tf.variable_scope('encoder'):\n",
    "            self.e_p = dict(\n",
    "                weights=[tf.Variable(xavier_init(ENCODER_SIZES[i], ENCODER_SIZES[i+1]),\n",
    "                                     name='weight_{}'.format(i))\n",
    "                         for i in range(ENCODER_DEPTH-1)],\n",
    "                biases=[tf.Variable(tf.zeros([ENCODER_SIZES[i+1]], dtype=tf.float32),\n",
    "                                    name='bias_{}'.format(i))\n",
    "                        for i in range(ENCODER_DEPTH-1)],\n",
    "                fc_activation=tf.nn.softplus,\n",
    "                out_mean=dict(\n",
    "                    weight=tf.Variable(xavier_init(ENCODER_SIZES[-1], LATENT_SIZE),\n",
    "                                       name='out_mean_weight'),\n",
    "                    bias=tf.Variable(tf.zeros([LATENT_SIZE], dtype=tf.float32),\n",
    "                                     name='out_mean_bias')\n",
    "                ),\n",
    "                out_log_sigma=dict(\n",
    "                    weight=tf.Variable(xavier_init(ENCODER_SIZES[-1], LATENT_SIZE),\n",
    "                                       name='out_log_sigma_weight'),\n",
    "                    bias=tf.Variable(tf.zeros([LATENT_SIZE], dtype=tf.float32),\n",
    "                                     name='out_mean_bias')\n",
    "                )\n",
    "            )\n",
    "        with tf.variable_scope('s-decoder'):\n",
    "            self.s_d_p = dict(\n",
    "                weights=[tf.Variable(xavier_init(SINGLE_DECODER_SIZES[i], SINGLE_DECODER_SIZES[i+1]),\n",
    "                                     name='weight_{}'.format(i))\n",
    "                         for i in range(SINGLE_DECODER_DEPTH-1)],\n",
    "                biases=[tf.Variable(tf.zeros([SINGLE_DECODER_SIZES[i+1]], dtype=tf.float32),\n",
    "                                    name='bias_{}'.format(i))\n",
    "                        for i in range(SINGLE_DECODER_DEPTH-1)],\n",
    "                fc_activation=tf.nn.softplus,\n",
    "                out_mean=dict(\n",
    "                    weight=tf.Variable(xavier_init(SINGLE_DECODER_SIZES[-1], SINGLE_DECODER_OUTPUT_LENGTH),\n",
    "                                       name='out_mean_weight'),\n",
    "                    bias=tf.Variable(tf.zeros([SINGLE_DECODER_OUTPUT_LENGTH], dtype=tf.float32),\n",
    "                                     name='out_mean_bias')\n",
    "                )\n",
    "            )\n",
    "        with tf.variable_scope('d-decoder'):\n",
    "            self.d_d_p = dict(\n",
    "                weights=[tf.Variable(xavier_init(DOUBLE_DECODER_SIZES[i], DOUBLE_DECODER_SIZES[i+1]),\n",
    "                                     name='weight_{}'.format(i))\n",
    "                         for i in range(DOUBLE_DECODER_DEPTH-1)],\n",
    "                biases=[tf.Variable(tf.zeros([DOUBLE_DECODER_SIZES[i+1]]), name='bias_{}'.format(i))\n",
    "                        for i in range(DOUBLE_DECODER_DEPTH-1)],\n",
    "                fc_activation=tf.nn.softplus,\n",
    "                out_mean=dict(\n",
    "                    weight=tf.Variable(xavier_init(DOUBLE_DECODER_SIZES[-1], DOUBLE_DECODER_OUTPUT_LENGTH),\n",
    "                                       name='out_mean_weight'),\n",
    "                    bias=tf.Variable(\n",
    "                        tf.zeros([DOUBLE_DECODER_OUTPUT_LENGTH], dtype=tf.float32), name='out_mean_bias')\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, learning_rate, training_epochs, display_step=1):\n",
    "    elaine = ELAINE(batch_size=batch_size, learning_rate=learning_rate)\n",
    "#     writer = tf.summary.FileWriter('model/elaine/', elaine.sess.graph)\n",
    "    \n",
    "    batch_one_epoch = int(edge_attr_matrix.shape[0] / batch_size) + 1\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        edge_attr_r = edge_attr_norm.sample(frac=1)\n",
    "        for i in range(batch_one_epoch):\n",
    "            edge_batch = next_batch(edge_attr_r, batch_size, (epoch + 1) * i)\n",
    "            cost = elaine.partial_fit(edge_batch)\n",
    "            avg_cost += cost / edge_attr_matrix.shape[0] * batch_size\n",
    "            if i % 100 == 0:\n",
    "                print('    Batch: %04d, cost: %8.5f' % (i+1, cost))\n",
    "        if epoch % display_step == 0:\n",
    "            print('>===Epoch: %04d, cost: %8.5f===<' % (epoch+1, avg_cost))\n",
    "    return elaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elaine = train(50, 0.0001, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elaine.transform(neig_norm.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "709px",
    "left": "1069px",
    "right": "20px",
    "top": "108px",
    "width": "734px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
